{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b954b97",
   "metadata": {},
   "source": [
    "# Twitter Data Processing Pipeline with Dask and Incremental Writes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4621abb",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook provides a complete pipeline for processing tweet data, optimized for large datasets using Dask. \n",
    "It also includes incremental writes to a CSV file to save progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6497653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "from typing import Dict, List, Any\n",
    "import logging\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3401f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "# warnings.filterwarnings(action='ignore', category=FutureWarning, module='pyspark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc794cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.basicConfig(filename='processing.log', level=logging.INFO)\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Twitter Data Processing\") \\\n",
    "#     .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72bc5f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_name = 'streamV2_tweetnet_2023-06'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "950d6673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSONL file\n",
    "# df = pd.read_json(f'../data/{data_name}.jsons', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "926c6c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation function\n",
    "def extract_fields(json_obj):\n",
    "    tweet_id = json_obj.get('tweet_id', '')\n",
    "    tweet_type = json_obj.get('tweet_type', '')\n",
    "    hashtags = json_obj.get('hashtags', [])\n",
    "    mentions = json_obj.get('mentions', [])\n",
    "    return {\n",
    "        'tweet_id': tweet_id,\n",
    "        'tweet_type': tweet_type,\n",
    "        'hashtags': hashtags,\n",
    "        'mentions': mentions\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c882ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API call\n",
    "def fetch_additional_info(tweet_id):\n",
    "    url = \"https://cdn.syndication.twimg.com/tweet-result\"\n",
    "    querystring = {\"id\": tweet_id, \"lang\": \"en\", \"token\": \"x\"}\n",
    "    payload = \"\"\n",
    "    headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/114.0\",\n",
    "    \"Accept\": \"*/*\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Origin\": \"https://platform.twitter.com\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Referer\": \"https://platform.twitter.com/\",\n",
    "    \"Sec-Fetch-Dest\": \"empty\",\n",
    "    \"Sec-Fetch-Mode\": \"cors\",\n",
    "    \"Sec-Fetch-Site\": \"cross-site\",\n",
    "    \"Pragma\": \"no-cache\",\n",
    "    \"Cache-Control\": \"no-cache\",\n",
    "    \"TE\": \"trailers\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.request(\"GET\", url, data=payload, headers=headers, params=querystring)\n",
    "        if response.status_code != 200:\n",
    "            # print(f\"Failed to fetch additional info for tweet_id {tweet_id}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.error(f'Failed to fetch additional info for tweet_id {tweet_id}')\n",
    "        return None\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bda53fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_response_test = fetch_additional_info(1664013863526735874)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "143db3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_api_response(api_response):\n",
    "    if not api_response:\n",
    "        return {}\n",
    "    try:\n",
    "        parsed_data = json.loads(api_response)\n",
    "    except json.JSONDecodeError:\n",
    "        logging.error(f'Failed parse_api_response {api_response}')\n",
    "        return {}\n",
    "    \n",
    "    lang = parsed_data.get('lang', '')\n",
    "    favorite_count = parsed_data.get('favorite_count', 0)\n",
    "    created_at = parsed_data.get('created_at', '')\n",
    "    text = parsed_data.get('text', '')\n",
    "    parent_tweet_id = parsed_data.get('parent', {}).get('id_str', '')\n",
    "    \n",
    "    return {\n",
    "        'lang': lang,\n",
    "        'favorite_count': favorite_count,\n",
    "        'created_at': created_at,\n",
    "        'text': text,\n",
    "        'parent_tweet_id': parent_tweet_id\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34f035dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_api_response_test = parse_api_response(api_response_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad8f4680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell not used\n",
    "\n",
    "# # Initialize a CSV writer and write the header\n",
    "# with open(f'output_{data_name}.csv', 'w', newline='') as f:\n",
    "#     writer = csv.DictWriter(f, fieldnames=['tweet_id', 'tweet_type', 'hashtags', 'mentions', 'lang', 'favorite_count', 'created_at', 'text', 'parent_tweet_id'])\n",
    "#     writer.writeheader()\n",
    "\n",
    "# Function to write a single row to the CSV file\n",
    "def write_row_to_csv(row):\n",
    "    try:\n",
    "        with open('output_{data_name}.csv', 'a', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['tweet_id', 'tweet_type', 'hashtags', 'mentions', 'lang', 'favorite_count', 'created_at', 'text', 'parent_tweet_id'])\n",
    "            writer.writerow(row)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to write row to CSV: {e}\")\n",
    "\n",
    "\n",
    "# Function to process a single JSON object (this includes the API call)\n",
    "def process_json_object(json_obj):\n",
    "    try: \n",
    "        # Extract initial fields\n",
    "        row = extract_fields(json_obj)\n",
    "        \n",
    "        # Fetch additional info from API (You'll have to add your API logic)\n",
    "        api_response = fetch_additional_info(row['tweet_id'])\n",
    "        \n",
    "        # Parse the API response\n",
    "        additional_info = parse_api_response(api_response)\n",
    "        \n",
    "        # Merge initial data and additional info\n",
    "        row.update(additional_info)\n",
    "        \n",
    "        # Write the row to CSV\n",
    "        write_row_to_csv(row)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process JSON object: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e25a8680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_write_csv(df: pd.DataFrame, output_path: str, data_name: str):\n",
    "    file_name = os.path.join(output_path, f'output_{data_name}.csv')\n",
    "    try:\n",
    "        df.to_csv(file_name, mode='a', index=False, header=False)\n",
    "    except Exception as e:\n",
    "        logging.error(f'Failed to write chunk {e}')\n",
    "        \n",
    "\n",
    "# Define a function to process a chunk of data\n",
    "def process_chunk(df_chunk: pd.DataFrame, output_path: str, data_name: str):\n",
    "    logging.info(f'Processing chunk entered')\n",
    "    results = []\n",
    "    for idx, row in df_chunk.iterrows():\n",
    "        row_dict = row.to_dict()\n",
    "        logging.info(f'Processing tweet id: {row_dict[\"tweet_id\"]}')\n",
    "        api_response = fetch_additional_info(row_dict['tweet_id'])\n",
    "        additional_info = parse_api_response(api_response)\n",
    "        row_dict.update(additional_info)\n",
    "        # Convert hashtags and mentions array to a comma-separated string\n",
    "        row_dict['hashtags'] = ','.join(row_dict['hashtags']) if isinstance(row_dict['hashtags'], (list, tuple)) else ''\n",
    "        row_dict['mentions'] = ','.join(row_dict['mentions']) if isinstance(row_dict['mentions'], (list, tuple)) else ''\n",
    "        results.append(row_dict)\n",
    "    result_df = pd.DataFrame(results)\n",
    "    # Filter the DataFrame to only include the columns specified in the schema\n",
    "    result_df = result_df[['tweet_id', 'tweet_type', 'hashtags', 'mentions', 'lang', 'favorite_count', 'created_at', 'text', 'parent_tweet_id']]\n",
    "    custom_write_csv(result_df, output_path, data_name)  # Pass output_path and data_name to custom_write_csv\n",
    "\n",
    "def process_data_in_parallel(df, output_path: str, data_name: str):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        chunks = np.array_split(df, 10)\n",
    "        # Use a lambda function to pass the output_path and data_name arguments to process_chunk\n",
    "        executor.map(lambda chunk: process_chunk(chunk, output_path, data_name), chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03475ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder_path = './test_Folder'\n",
    "output_folder_path = '../data/output/test_Folder'\n",
    "os.makedirs(output_folder_path, exist_ok=True)  # Create output folder if it doesn't exist\n",
    "\n",
    "def process_file(file_path, output_path):\n",
    "    # Extract data_name from the file path\n",
    "    data_name = os.path.basename(file_path).replace('.jsons', '')\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "    \n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Write the header to the output file\n",
    "    output_file = os.path.join(output_path, f'output_{data_name}.csv')\n",
    "    header_df = pd.DataFrame(columns=['tweet_id', 'tweet_type', 'hashtags', 'mentions', 'lang', 'favorite_count', 'created_at', 'text', 'parent_tweet_id'])\n",
    "    header_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Process each chunk in parallel\n",
    "    process_data_in_parallel(df, output_path, data_name)\n",
    "\n",
    "def process_all_files_in_folder(folder_path, output_folder_path):\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.jsons'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            logging.info(f'Processing file: {file_name}')\n",
    "            process_file(file_path, output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff02aa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing file: streamV2_tweetnet_2023-06_1.jsons\n",
      "INFO:root:Processing chunk entered\n",
      "INFO:root:Processing tweet id: 1664013858896326656\n",
      "INFO:root:Processing chunk entered\n",
      "INFO:root:Processing chunk entered\n",
      "INFO:root:Processing chunk entered\n",
      "INFO:root:Processing chunk entered\n",
      "INFO:root:Processing tweet id: 1664013868543234048\n",
      "INFO:root:Processing chunk entered\n",
      "INFO:root:Processing tweet id: 1664013879880327168\n",
      "INFO:root:Processing chunk entered\n",
      "INFO:root:Processing chunk entered\n",
      "INFO:root:Processing tweet id: 1664013888923533312\n",
      "INFO:root:Processing chunk entered\n",
      "INFO:root:Processing tweet id: 1664013897005780992\n",
      "INFO:root:Processing chunk entered\n",
      "INFO:root:Processing tweet id: 1664013905071308800\n",
      "INFO:root:Processing tweet id: 1664013913027911680\n",
      "INFO:root:Processing tweet id: 1664013918501576704\n",
      "INFO:root:Processing tweet id: 1664013930677641216\n",
      "INFO:root:Processing tweet id: 1664013936839077888\n",
      "INFO:root:Processing tweet id: 1664013872343203840\n",
      "INFO:root:Processing tweet id: 1664013902110179328\n",
      "INFO:root:Processing tweet id: 1664013857646411776\n",
      "INFO:root:Processing tweet id: 1664013886171893760\n",
      "INFO:root:Processing tweet id: 1664013929780072448\n",
      "INFO:root:Processing tweet id: 1664013912319074304\n",
      "INFO:root:Processing tweet id: 1664013920846114816\n",
      "INFO:root:Processing tweet id: 1664013877732950016\n",
      "INFO:root:Processing tweet id: 1664013897852919808\n",
      "INFO:root:Processing tweet id: 1664013938176974848\n",
      "INFO:root:Processing tweet id: 1664013869424037888\n",
      "INFO:root:Processing tweet id: 1664013904400228352\n",
      "INFO:root:Processing tweet id: 1664013889124573184\n",
      "INFO:root:Processing tweet id: 1664013860578230272\n",
      "INFO:root:Processing tweet id: 1664013911774068736\n",
      "INFO:root:Processing tweet id: 1664013898503057408\n",
      "INFO:root:Processing tweet id: 1664013919638126592\n",
      "INFO:root:Processing tweet id: 1664013931080294400\n",
      "INFO:root:Processing tweet id: 1664013939879936000\n",
      "INFO:root:Processing tweet id: 1664013881902088192\n",
      "INFO:root:Processing tweet id: 1664013870648762368\n",
      "INFO:root:Processing tweet id: 1664013905457291264\n",
      "INFO:root:Processing tweet id: 1664013893214109696\n",
      "INFO:root:Processing tweet id: 1664013863526735872\n",
      "INFO:root:Processing tweet id: 1664013913908826112\n",
      "INFO:root:Processing tweet id: 1664013901019729920\n",
      "INFO:root:Processing tweet id: 1664013933156417536\n",
      "INFO:root:Processing tweet id: 1664013921009778688\n",
      "INFO:root:Processing tweet id: 1664013883797913600\n",
      "INFO:root:Processing tweet id: 1664013943419838464\n",
      "INFO:root:Processing tweet id: 1664013871865114624\n",
      "INFO:root:Processing tweet id: 1664013909051822080\n",
      "INFO:root:Processing tweet id: 1664013889653157888\n",
      "INFO:root:Processing tweet id: 1664013861442207744\n",
      "INFO:root:Processing tweet id: 1664013914680557568\n",
      "INFO:root:Processing tweet id: 1664013903016148992\n",
      "INFO:root:Processing tweet id: 1664013933420634112\n",
      "INFO:root:Processing tweet id: 1664013923077570560\n",
      "INFO:root:Processing tweet id: 1664013908422606848\n",
      "INFO:root:Processing tweet id: 1664013884167016448\n",
      "INFO:root:Processing tweet id: 1664013941960310784\n",
      "INFO:root:Processing tweet id: 1664013874897592320\n",
      "INFO:root:Processing tweet id: 1664013889623715840\n",
      "INFO:root:Processing tweet id: 1664013858292346880\n",
      "INFO:root:Processing tweet id: 1664013916953772032\n",
      "INFO:root:Processing tweet id: 1664013901619445760\n",
      "INFO:root:Processing tweet id: 1664013933135495168\n",
      "INFO:root:Processing tweet id: 1664013926651011072\n",
      "INFO:root:Processing tweet id: 1664013881256157184\n",
      "INFO:root:Processing tweet id: 1664013873156857856\n",
      "INFO:root:Processing tweet id: 1664013905251762176\n",
      "INFO:root:Processing tweet id: 1664013862885117952\n",
      "INFO:root:Processing tweet id: 1664013940907442176\n",
      "INFO:root:Processing tweet id: 1664013892769521664\n",
      "INFO:root:Processing tweet id: 1664013916811276288\n",
      "INFO:root:Processing tweet id: 1664013904979062784\n",
      "INFO:root:Processing tweet id: 1664013934431444992\n",
      "INFO:root:Processing tweet id: 1664013923073286144\n",
      "INFO:root:Processing tweet id: 1664013882422095872\n",
      "INFO:root:Processing tweet id: 1664013910414852096\n",
      "INFO:root:Processing tweet id: 1664013874096484352\n",
      "INFO:root:Processing tweet id: 1664013897450356736\n",
      "INFO:root:Processing tweet id: 1664013917167788032\n",
      "INFO:root:Processing tweet id: 1664013940907442176\n",
      "INFO:root:Processing tweet id: 1664013923014549504\n",
      "INFO:root:Processing tweet id: 1664013903372726272\n",
      "INFO:root:Processing tweet id: 1664013932573360128\n",
      "INFO:root:Processing tweet id: 1664013884418605056\n",
      "INFO:root:Processing tweet id: 1664013861836447744\n",
      "INFO:root:Processing tweet id: 1664013906694619136\n",
      "INFO:root:Processing tweet id: 1664013940907442176\n",
      "INFO:root:Processing tweet id: 1664013873186324480\n",
      "INFO:root:Processing tweet id: 1664013897056284672\n",
      "INFO:root:Processing tweet id: 1664013943419838464\n",
      "INFO:root:Processing tweet id: 1664013917838819328\n",
      "INFO:root:Processing tweet id: 1664013927892545536\n",
      "INFO:root:Processing tweet id: 1664013903762800640\n",
      "INFO:root:Processing tweet id: 1664013943419838464\n",
      "INFO:root:Processing tweet id: 1664013932749549568\n",
      "INFO:root:Processing tweet id: 1664013866039115776\n",
      "INFO:root:Processing tweet id: 1664013886750687232\n",
      "INFO:root:Processing tweet id: 1664013876969578496\n",
      "INFO:root:Processing tweet id: 1664013879674847232\n",
      "INFO:root:Processing tweet id: 1664013916786118656\n",
      "INFO:root:Processing tweet id: 1664013928525881344\n",
      "INFO:root:Processing tweet id: 1664013902110154752\n",
      "INFO:root:Processing tweet id: 1664013895818698752\n",
      "INFO:root:Processing tweet id: 1664013934414659584\n",
      "INFO:root:Processing tweet id: 1664013886222204928\n",
      "INFO:root:Processing tweet id: 1664013868148961280\n",
      "INFO:root:Processing tweet id: 1664013879054090240\n",
      "INFO:root:Processing tweet id: 1664013894048784384\n",
      "INFO:root:Processing tweet id: 1664013886855557120\n",
      "INFO:root:Processing tweet id: 1664013866039214080\n",
      "INFO:root:Processing file: streamV2_tweetnet_2023-06_0.jsons\n",
      "INFO:root:Processing chunk entered\n",
      "INFO:root:Processing chunk entered\n",
      "INFO:root:Processing tweet id: 1664013858896326656\n",
      "INFO:root:Processing chunk entered\n",
      "INFO:root:Processing chunk entered\n",
      "INFO:root:Processing chunk entered\n",
      "INFO:root:Processing tweet id: 1664013866039214080\n",
      "INFO:root:Processing chunk entered\n",
      "INFO:root:Processing chunk entered\n",
      "INFO:root:Processing chunk entered\n",
      "INFO:root:Processing chunk entered\n",
      "INFO:root:Processing chunk entered\n",
      "INFO:root:Processing tweet id: 1664013879674847232\n",
      "INFO:root:Processing tweet id: 1664013886750687232\n",
      "INFO:root:Processing tweet id: 1664013897450356736\n",
      "INFO:root:Processing tweet id: 1664013904979062784\n",
      "INFO:root:Processing tweet id: 1664013905251762176\n",
      "INFO:root:Processing tweet id: 1664013916811276288\n",
      "INFO:root:Processing tweet id: 1664013923073286144\n",
      "INFO:root:Processing tweet id: 1664013934431444992\n",
      "INFO:root:Processing tweet id: 1664013868543234048\n",
      "INFO:root:Processing tweet id: 1664013932573360128\n",
      "INFO:root:Processing tweet id: 1664013886222204928\n",
      "INFO:root:Processing tweet id: 1664013879054090240\n",
      "INFO:root:Processing tweet id: 1664013903372726272\n",
      "INFO:root:Processing tweet id: 1664013857646411776\n",
      "INFO:root:Processing tweet id: 1664013897056284672\n",
      "INFO:root:Processing tweet id: 1664013910414852096\n",
      "INFO:root:Processing tweet id: 1664013932749549568\n",
      "INFO:root:Processing tweet id: 1664013917167788032\n",
      "INFO:root:Processing tweet id: 1664013923014549504\n",
      "INFO:root:Processing tweet id: 1664013886855557120\n",
      "INFO:root:Processing tweet id: 1664013879880327168\n",
      "INFO:root:Processing tweet id: 1664013903762800640\n",
      "INFO:root:Processing tweet id: 1664013872343203840\n",
      "INFO:root:Processing tweet id: 1664013860578230272\n",
      "INFO:root:Processing tweet id: 1664013934414659584\n",
      "INFO:root:Processing tweet id: 1664013906694619136\n",
      "INFO:root:Processing tweet id: 1664013895818698752\n",
      "INFO:root:Processing tweet id: 1664013877732950016\n",
      "INFO:root:Processing tweet id: 1664013894048784384\n",
      "INFO:root:Processing tweet id: 1664013876969578496\n",
      "INFO:root:Processing tweet id: 1664013888923533312\n",
      "INFO:root:Processing tweet id: 1664013917838819328\n",
      "INFO:root:Processing tweet id: 1664013927892545536\n",
      "INFO:root:Processing tweet id: 1664013902110154752\n",
      "INFO:root:Processing tweet id: 1664013863526735872\n",
      "INFO:root:Processing tweet id: 1664013869424037888\n",
      "INFO:root:Processing tweet id: 1664013913027911680\n",
      "INFO:root:Processing tweet id: 1664013936839077888\n",
      "INFO:root:Processing tweet id: 1664013897005780992\n",
      "INFO:root:Processing tweet id: 1664013881902088192\n",
      "INFO:root:Processing tweet id: 1664013916786118656\n",
      "INFO:root:Processing tweet id: 1664013886171893760\n",
      "INFO:root:Processing tweet id: 1664013938176974848\n",
      "INFO:root:Processing tweet id: 1664013897852919808\n",
      "INFO:root:Processing tweet id: 1664013870648762368\n",
      "INFO:root:Processing tweet id: 1664013918501576704\n",
      "INFO:root:Processing tweet id: 1664013928525881344\n",
      "INFO:root:Processing tweet id: 1664013905071308800\n",
      "INFO:root:Processing tweet id: 1664013861442207744\n",
      "INFO:root:Processing tweet id: 1664013939879936000\n",
      "INFO:root:Processing tweet id: 1664013912319074304\n",
      "INFO:root:Processing tweet id: 1664013883797913600\n",
      "INFO:root:Processing tweet id: 1664013920846114816\n",
      "INFO:root:Processing tweet id: 1664013902110179328\n",
      "INFO:root:Processing tweet id: 1664013889124573184\n",
      "INFO:root:Processing tweet id: 1664013943419838464\n",
      "INFO:root:Processing tweet id: 1664013898503057408\n",
      "INFO:root:Processing tweet id: 1664013871865114624\n",
      "INFO:root:Processing tweet id: 1664013930677641216\n",
      "INFO:root:Processing tweet id: 1664013893214109696\n",
      "INFO:root:Processing tweet id: 1664013858292346880\n",
      "INFO:root:Processing tweet id: 1664013941960310784\n",
      "INFO:root:Processing tweet id: 1664013901019729920\n",
      "INFO:root:Processing tweet id: 1664013874897592320\n",
      "INFO:root:Processing tweet id: 1664013884167016448\n",
      "INFO:root:Processing tweet id: 1664013911774068736\n",
      "INFO:root:Processing tweet id: 1664013919638126592\n",
      "INFO:root:Processing tweet id: 1664013904400228352\n",
      "INFO:root:Processing tweet id: 1664013929780072448\n",
      "INFO:root:Processing tweet id: 1664013889653157888\n",
      "INFO:root:Processing tweet id: 1664013903016148992\n",
      "INFO:root:Processing tweet id: 1664013862885117952\n",
      "INFO:root:Processing tweet id: 1664013931080294400\n",
      "INFO:root:Processing tweet id: 1664013881256157184\n",
      "INFO:root:Processing tweet id: 1664013940907442176\n",
      "INFO:root:Processing tweet id: 1664013873156857856\n",
      "INFO:root:Processing tweet id: 1664013861836447744\n",
      "INFO:root:Processing tweet id: 1664013921009778688\n",
      "INFO:root:Processing tweet id: 1664013913908826112\n",
      "INFO:root:Processing tweet id: 1664013882422095872\n",
      "INFO:root:Processing tweet id: 1664013905457291264\n",
      "INFO:root:Processing tweet id: 1664013901619445760\n",
      "INFO:root:Processing tweet id: 1664013889623715840\n",
      "INFO:root:Processing tweet id: 1664013874096484352\n",
      "INFO:root:Processing tweet id: 1664013933156417536\n",
      "INFO:root:Processing tweet id: 1664013923077570560\n",
      "INFO:root:Processing tweet id: 1664013866039115776\n",
      "INFO:root:Processing tweet id: 1664013914680557568\n",
      "INFO:root:Processing tweet id: 1664013873186324480\n",
      "INFO:root:Processing tweet id: 1664013884418605056\n",
      "INFO:root:Processing tweet id: 1664013933420634112\n",
      "INFO:root:Processing tweet id: 1664013916953772032\n",
      "INFO:root:Processing tweet id: 1664013909051822080\n",
      "INFO:root:Processing tweet id: 1664013892769521664\n",
      "INFO:root:Processing tweet id: 1664013926651011072\n",
      "INFO:root:Processing tweet id: 1664013868148961280\n",
      "INFO:root:Processing tweet id: 1664013933135495168\n",
      "INFO:root:Processing tweet id: 1664013908422606848\n"
     ]
    }
   ],
   "source": [
    "process_all_files_in_folder(input_folder_path, output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d7f69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Call the function to process all files in the specified folder\n",
    "# file_path = \"./test_Folder/streamV2_tweetnet_2023-06_0.jsons\"\n",
    "# # Extract data_name from the file path\n",
    "# data_name = os.path.basename(file_path).replace('.jsons', '')\n",
    "# df = pd.read_json(file_path, lines=True)\n",
    "# output_file = os.path.join(output_folder_path, f'output_{data_name}.csv')  # Update this line to use output_folder_path\n",
    "\n",
    "#     # Write the header to the output file\n",
    "# header_df = pd.DataFrame(columns=['tweet_id', 'tweet_type', 'hashtags', 'mentions', 'lang', 'favorite_count', 'created_at', 'text', 'parent_tweet_id'])\n",
    "# header_df.to_csv(output_file, index=False)\n",
    "# process_chunk(df, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a582fffe",
   "metadata": {},
   "source": [
    "## For other one-time purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d677ae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split .jsons file into several equal parts\n",
    "# import os\n",
    "# data_name_to_be_splitted = \"streamV2_tweetnet_2023-03\"\n",
    "\n",
    "# # Define the new folder name\n",
    "# new_folder = f\"{data_name_to_be_splitted}_splitted\"\n",
    "\n",
    "# # Ensure the folder exists, create it if not\n",
    "# os.makedirs(new_folder, exist_ok=True)\n",
    "\n",
    "# def split_file(large_file_path, lines_per_file):\n",
    "#     with open(large_file_path, 'r') as file:\n",
    "#         file_count = 0\n",
    "#         current_line_count = 0\n",
    "#         # Modify the file path to include the new folder\n",
    "#         current_file = open(f'{new_folder}/{data_name_to_be_splitted}_{file_count}.jsons', 'w')\n",
    "#         for line in file:\n",
    "#             if current_line_count < lines_per_file:\n",
    "#                 current_file.write(line)\n",
    "#                 current_line_count += 1\n",
    "#             else:\n",
    "#                 current_file.close()\n",
    "#                 file_count += 1\n",
    "#                 # Modify the file path to include the new folder\n",
    "#                 current_file = open(f'{new_folder}/{data_name_to_be_splitted}_{file_count}.jsons', 'w')\n",
    "#                 current_file.write(line)\n",
    "#                 current_line_count = 1\n",
    "#         current_file.close()\n",
    "\n",
    "# # Call the function as usual\n",
    "# split_file(f'../data/{data_name_to_be_splitted}.jsons', 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5125d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST Usage:\n",
    "# folder_path = './test_Folder'\n",
    "# output_folder_path = '../data/output_files'\n",
    "# os.makedirs(output_folder_path, exist_ok=True)  # Create output folder if it doesn't exist\n",
    "\n",
    "# def process_file(file_path):\n",
    "#     # Extract data_name from the file path\n",
    "#     data_name = os.path.basename(file_path).replace('.jsons', '')\n",
    "#     df = pd.read_json(file_path, lines=True)\n",
    "#     output_file = os.path.join(output_folder_path, f'output_{data_name}.csv')  # Update this line to use output_folder_path\n",
    "\n",
    "#     # Write the header to the output file\n",
    "#     header_df = pd.DataFrame(columns=['tweet_id', 'tweet_type', 'hashtags', 'mentions', 'lang', 'favorite_count', 'created_at', 'text', 'parent_tweet_id'])\n",
    "#     header_df.to_csv(output_file, index=False)\n",
    "    \n",
    "#     # Process each chunk in parallel\n",
    "#     process_data_in_parallel(df, output_file)\n",
    "\n",
    "# def process_all_files_in_folder(folder_path):\n",
    "#     for file_name in os.listdir(folder_path):\n",
    "#         if file_name.endswith('.jsons'):\n",
    "#             file_path = os.path.join(folder_path, file_name)\n",
    "#             print(f'Processing file: {file_name}')  # Print the file name for tracking\n",
    "#             process_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6830a91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to process all files in the specified folder\n",
    "process_all_files_in_folder(folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
